{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e4b37a",
   "metadata": {
    "id": "b1e4b37a"
   },
   "source": [
    "**Practical session n°1**\n",
    "In part I:\n",
    "- Learning a perceptron through stochastic gradient descent.\n",
    "- Multi-layer perceptron.\n",
    "- Cost function adapted to classification tasks.\n",
    "\n",
    "In part II:\n",
    "- Other basic building blocks of deep networks: convolutional layers and non-linearities.\n",
    "- Analysis of a standard deep network trained on imagenet.\n",
    "\n",
    "In part III:\n",
    "- Training a CNN on MNIST. Comparison with a multi-layer perceptron.\n",
    "- Initialization methods, regularization methods.\n",
    "\n",
    "In part IV:\n",
    "- Training on a graphics card.\n",
    "- Improving gradient descent: SGD with momentum and progressive learning rate reduction (scheduler).\n",
    "- Transfer learning: fine-tuning and freezing.\n",
    "\n",
    "Duration: 4 h\n",
    "\n",
    "**Part I**\n",
    "\n",
    "This part introduces neural networks through a presentation of the perceptron. It is also an opportunity to familiarize yourself with PyTorch commands. PyTorch is one of the three most widely used libraries for deep learning, along with Keras and TensorFlow (Keras is built on top of Tensorflow).\n",
    "\n",
    "\"Deep learning\" is, by definition, the learning of \"deep\" neural networks through stochastic gradient descent. By \"deep,\" we mean networks composed of a succession of \"layers\" of neurons.\n",
    "\n",
    "The basic building blocks that allow us to construct these layers are coded in the torch.nn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88495c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b88495c8",
    "outputId": "3f147008-7b79-49e5-b677-fd004a261f0c"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0dcd41",
   "metadata": {
    "id": "1b0dcd41"
   },
   "source": [
    "**A.** First, let's revisit the perceptron. To introduce learning through stochastic gradient descent, we will address a simple binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72725a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "b72725a8",
    "outputId": "02da5f34-08b5-4f3b-d771-a0d026b43f4d"
   },
   "outputs": [],
   "source": [
    "# Data to separate:\n",
    "n = 100\n",
    "std = 0.5\n",
    "\n",
    "# Sample 1:\n",
    "mean0 = torch.tensor((-1., -1.))\n",
    "ech0 = mean0 + std * torch.randn(n, 2)\n",
    "\n",
    "# Sample 2:\n",
    "mean1 = torch.tensor((1., 1.))\n",
    "ech1 = mean1 + std * torch.randn(n, 2)\n",
    "\n",
    "echs = [ech0, ech1]\n",
    "\n",
    "# Scatter plot:\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis([-3, 3, -3, 3])\n",
    "\n",
    "plt.title('data')\n",
    "colors = ['b', 'r']\n",
    "labels = ['0', '1']\n",
    "\n",
    "for i, ech in enumerate(echs):\n",
    "    x, y = ech.numpy()[:, 0], ech.numpy()[:, 1]\n",
    "    ax.scatter(x, y, color=colors[i])\n",
    "\n",
    "plt.legend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43354804",
   "metadata": {
    "id": "43354804"
   },
   "source": [
    "A simple perceptron (single neuron) consists of two parts: a linear part containing a dot product and a \"bias\" (b) and a non-linear part, the activation function (A):\n",
    "\\begin{equation*}\n",
    " f(x; \\omega,b) = \\mathcal{A}({\\sum} \\omega_i x_i  + b )  \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "The class P1 below codes for perceptrons defined on R^2 and whose activation function is a sigmoid:\n",
    "\\begin{equation*}\n",
    "\\mathcal{A}(y) = \\dfrac{1}{1+e^{-y}}\n",
    "\\end{equation*}\n",
    "The sigmoid function is in the range [0, 1]. Therefore, the neuron's output can be interpreted as the probability of belonging to the first of the two classes. In the P1 class, the neuron actually returns a vector of \"probabilities\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8986f9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8986f9b",
    "outputId": "cc3e2be5-749b-4e2c-ed77-99e4a8fc0a34"
   },
   "outputs": [],
   "source": [
    "class P1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(P1, self).__init__()\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dot product and bias\n",
    "        x = self.fc(x)\n",
    "        # Activation\n",
    "        x = x.sigmoid()\n",
    "        # Vector of \"probabilities\" (cat: concatenation)\n",
    "        x = torch.cat((x, 1 - x), dim=1)\n",
    "        return x\n",
    "\n",
    "model = P1()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca91dfe",
   "metadata": {
    "id": "4ca91dfe"
   },
   "source": [
    "In the next cell, basic commands are given to access the weights of an instance of P1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd27f40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dd27f40",
    "outputId": "4765568e-ef65-4f8e-df54-f66599356e22"
   },
   "outputs": [],
   "source": [
    "# Initialization (fantasy) of weights:\n",
    "model.fc.weight[0, 0].data.fill_(-0.1)\n",
    "model.fc.weight[0, 1].data.fill_(0.5)\n",
    "model.fc.bias.data.fill_(-1)\n",
    "\n",
    "# Retrieving weights:\n",
    "fc = model.fc\n",
    "weights = fc.weight.data.squeeze(dim=0)\n",
    "bias = fc.bias.data\n",
    "\n",
    "print(weights)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417aad92",
   "metadata": {
    "id": "417aad92"
   },
   "source": [
    "**Exercise 1**: Verify on an example that $f_c(x) = \\sum \\omega_i x_i  + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22a4d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "cf22a4d2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f61797496e6ebc3f98ba5ea49e10e07",
     "grade": false,
     "grade_id": "exercise-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "5f658993-711b-4ad2-ff9f-df941d433730",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70518cbf",
   "metadata": {
    "id": "70518cbf"
   },
   "source": [
    "To complete the definition of the perceptron, a decision rule is needed. This rule is natural: for $f(x; \\omega, b) = (p_0, p_1)$, we choose class 0 if $p_0 > p_1$.\n",
    "\n",
    "We can plot the boundary that delimits the model's decision regions. Provide its equation in the form $x_1 = \\alpha x_0 + \\beta$ where $\\alpha$ and $\\beta$ depend on $\\omega$ and $b$.\n",
    "\n",
    "**Exercise 2:** Complete the code below to plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2446c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "deletable": false,
    "id": "6c2446c6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f1ca095ac2551b73a3153c32ea2ff07",
     "grade": false,
     "grade_id": "exercise-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "05df54a7-a6a7-47d7-d57e-dfcb429413b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def traceFrontiere(weights, bias, ax, interval=[-10, 10], color='black'):\n",
    "    x0 = np.arange(interval[0], interval[1], 0.01)\n",
    "    # x1 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ax.plot(x0, x1, color=color)\n",
    "\n",
    "traceFrontiere(weights.numpy(), bias.numpy(), ax)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81542b9",
   "metadata": {
    "id": "b81542b9"
   },
   "source": [
    "To train the perceptron to correctly separate the classes, we will use stochastic gradient descent with mini-batches. For this, we need to present the model with (*input*, *target*) pairs in a **random** order. In PyTorch, this selection is done using two objects:\n",
    "- A *Dataset* class\n",
    "- A *Dataloader* class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d8715",
   "metadata": {
    "id": "923d8715"
   },
   "source": [
    "A PyTorch Dataset contains a method for accessing data. The following class provides a rudimentary example. Later on, we will integrate data loading, normalization, and data augmentation steps into our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f26701",
   "metadata": {
    "id": "00f26701"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class FirstDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.targets.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):  # idx is an index called by the loader\n",
    "        x = self.inputs[idx, :]\n",
    "        t = self.targets[idx]\n",
    "        return x, t  # (input, target) pair\n",
    "\n",
    "inputs_train = torch.cat(echs, dim=0)\n",
    "targets_train = torch.cat((torch.zeros(n), torch.ones(n)), dim=0).long()\n",
    "ds1 = FirstDataset(inputs_train, targets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e95810",
   "metadata": {
    "id": "96e95810"
   },
   "source": [
    "A **loader** is a Python iterable (like lists, dictionaries, etc.) that we parameterize by batch size and data selection method (with or without replacement, weighting, etc). In particular, with the shuffle = True option, the data is reshuffled at every epoch and num_workers represents how many subprocesses to use for data loading, 0 means that the data will be loaded in the main process (generally interesting to anticipate data loading, to speed up training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee1e46",
   "metadata": {
    "id": "6fee1e46"
   },
   "outputs": [],
   "source": [
    "loader1 = DataLoader(ds1, batch_size=10, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14640f6a",
   "metadata": {
    "id": "14640f6a"
   },
   "source": [
    "In the following figure, we represent a first randomly drawn batch of points. Each time the window is executed, a new batch of points is drawn until exhaustion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf4375",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "ebbf4375",
    "outputId": "30fa1522-1a49-43bc-d535-dca23c9f41ae"
   },
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.axis([-3, 3, -3, 3])\n",
    "\n",
    "# Drawing a batch of ten points\n",
    "inputs, targets = next(iter(loader1))\n",
    "x, y = inputs.numpy()[:, 0], inputs.numpy()[:, 1]\n",
    "cs = [colors[targets[i]] for i in range(len(targets))]\n",
    "ax2.scatter(x, y, color=cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3051a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "5e3051a2",
    "outputId": "aba8f43a-0d9a-4294-861f-cd402d9d9073"
   },
   "outputs": [],
   "source": [
    "# Iterating over the dataset\n",
    "inputs, targets = next(iter(loader1))\n",
    "x, y = inputs.numpy()[:, 0], inputs.numpy()[:, 1]\n",
    "cs = [colors[targets[i]] for i in range(len(targets))]\n",
    "ax2.scatter(x, y, color=cs)\n",
    "fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2dcbbb",
   "metadata": {
    "id": "4a2dcbbb"
   },
   "source": [
    "For each available batch, we calculate the model's error using a loss function. This loss function penalizes the differences between the network's outputs (here, pairs $(p_0, p_1)$) and the ground truth (here, a class $c \\in \\{ 0 ; 1 \\}$).\n",
    "\n",
    "In classification, we generally use the negative log likelihood. For a batch point, it is defined as:\n",
    "\n",
    "$\\mathcal{L}((p_0, p_1), c) = - ln(p_c)$\n",
    "\n",
    "This quantity is averaged over each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24c290",
   "metadata": {
    "id": "3f24c290"
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets, show=False):\n",
    "    # all -log(p)\n",
    "    outputs = -torch.log(outputs)\n",
    "    # all -log(pc)\n",
    "    tensor_of_losses = torch.gather(outputs, 1, targets.unsqueeze(dim=1))\n",
    "    # average of -log(pc)\n",
    "    loss = tensor_of_losses.mean()\n",
    "\n",
    "    if show:\n",
    "        print(outputs)\n",
    "        print(targets)\n",
    "        print(tensor_of_losses)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ca365",
   "metadata": {
    "id": "6d7ca365"
   },
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a632e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b7a632e",
    "outputId": "37c57826-0937-47f7-a33a-d2eba05fb6ab"
   },
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(loader1))\n",
    "l = loss_fn(model(inputs), targets, show=True)\n",
    "\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c9f11",
   "metadata": {
    "id": "409c9f11"
   },
   "source": [
    "For each batch, we calculate the derivatives $\\dfrac{\\partial \\mathcal{L_{batch}}}{\\partial{\\omega_i}}$ where $\\mathcal{L_{batch}}$ is the average of the cost function over the batch.\n",
    "\n",
    "PyTorch keeps track of each operation performed with the weights so that it can apply the usual rules of derivation. This calculation is launched with the *.backward* method. The derivatives are stored with the weights and can be accessed with *.grad*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021822c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "021822c0",
    "outputId": "2b7c70db-0884-422f-d1df-f9a80ec854f9"
   },
   "outputs": [],
   "source": [
    "w = model.fc.weight  # [0,0]\n",
    "print('before backward:' + str(w.grad))\n",
    "\n",
    "l.backward()\n",
    "\n",
    "print('after backward:' + str(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85922688",
   "metadata": {
    "id": "85922688"
   },
   "source": [
    "Last step: updating the weights. For this, many methods are available. We specify the chosen method through the PyTorch \"optimizer\" object. The simplest is written:\n",
    "\n",
    "$w_i := w_i - lr \\times \\dfrac{\\partial \\mathcal{L_{batch}}}{\\partial{\\omega_i}}$  (2)\n",
    "\n",
    "The learning rate ($lr$) controls the amplitude of the increments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612c9d9",
   "metadata": {
    "id": "b612c9d9"
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "# Two commonly used descent methods:\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)  # corresponds to equation (2)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a65cc",
   "metadata": {
    "id": "cf9a65cc"
   },
   "source": [
    "In the next cell, we train the perceptron. With each new execution, the dataset is traversed twice (two \"epochs\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756d65f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "5756d65f",
    "outputId": "b0bd8624-73b5-4b7c-f30d-2df064b48e2c"
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    print(epoch)\n",
    "    # random traversal of the dataset\n",
    "    for x, targets in loader1:\n",
    "        # zeroing gradients\n",
    "        optimizer.zero_grad()\n",
    "        # calculation of (p0, p1)\n",
    "        output = model(x)\n",
    "        # calculation of the error\n",
    "        l = loss_fn(output, targets)\n",
    "        # calculation of gradients\n",
    "        l.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "\n",
    "    # Plotting the hyperplane\n",
    "    fc = model.fc\n",
    "    weights = fc.weight.data.squeeze(dim=0)\n",
    "    bias = fc.bias.data\n",
    "    traceFrontiere(weights.numpy(), bias.numpy(), ax)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f458025",
   "metadata": {
    "id": "3f458025"
   },
   "source": [
    "**Exercise 3-1**: Complete the following code to plot the gradients in the $\\omega_0, \\omega_1$ weight space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3b29e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "deletable": false,
    "id": "c7c3b29e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ee62aef14a34c3e87c4ddd0f5a6d03d",
     "grade": false,
     "grade_id": "exercise-3-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "a570c640-b808-4a09-d4a8-75e8ab24307d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization of weights:\n",
    "model.fc.weight[0, 0].data.fill_(-0.1)\n",
    "model.fc.weight[0, 1].data.fill_(0.5)\n",
    "model.fc.bias.data.fill_(-1)\n",
    "\n",
    "fig3 = plt.figure()\n",
    "ax3 = fig3.add_subplot(111)\n",
    "ax3.axis([-1.5, 0, -1, 0.6])\n",
    "\n",
    "loader1 = DataLoader(ds1, batch_size=10, shuffle=True)\n",
    "lr = 0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(25):\n",
    "    for x, label in loader1:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        l = loss_fn(output, label)\n",
    "        l.backward()\n",
    "\n",
    "        # plotting vectors:\n",
    "        # weights = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # ax3.scatter(...)\n",
    "        # ax3.arrow(...)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b7141",
   "metadata": {
    "id": "757b7141"
   },
   "source": [
    "**Exercise 3-2:** Determine the accuracy of the classifier on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9056a67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "a9056a67",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08db64521cb6ad3687ba520557dfa517",
     "grade": false,
     "grade_id": "exercise-3-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "97fe7675-4d5c-44fa-fc6a-0c474cefd804",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy on the training set:\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f\"Accuracy:{acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9216817",
   "metadata": {
    "id": "c9216817"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "A model of the class *P1* corresponds to a class of statistical models widely used with predictors of small dimensions: logistic regression.\n",
    "\n",
    "This model is used to **explain and predict** the value of a binary qualitative variable.\n",
    "\n",
    "Let Z be a random variable with values in {c_1, c_2}.\n",
    "Logistic regression with respect to the predictor $X = (X_1, X_2, ...)$ is written:\n",
    "\n",
    "$$ ℙ(Z = c_1 | X ) = \\sigma (\\sum \\omega_i X_i + b ) $$\n",
    "\n",
    "Where $\\sigma$ is the sigmoid function. In the context of logistic regression, the weights $\\omega_i$ are obtained by **maximum likelihood**.\n",
    "\n",
    "**B.** Now, let's consider the question of separating more complex sets of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0621505f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "0621505f",
    "outputId": "bf32e66d-cfb2-4391-e1d4-8d0e5492cecb"
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "std = 0.5\n",
    "# sample 0:\n",
    "meana = torch.tensor((-1., -1.))\n",
    "echa = meana + std * torch.randn(n, 2)\n",
    "meanb = torch.tensor((1., 1.))\n",
    "echb = meanb + std * torch.randn(n, 2)\n",
    "\n",
    "ech0 = torch.cat([echa, echb])\n",
    "\n",
    "# sample :\n",
    "meanc = torch.tensor((1., -1.))\n",
    "echc = meanc + std * torch.randn(n, 2)\n",
    "meand = torch.tensor((-1., 1.))\n",
    "echd = meand + std * torch.randn(n, 2)\n",
    "\n",
    "ech1 = torch.cat([echc, echd])\n",
    "\n",
    "echs2 = [ech0, ech1]\n",
    "\n",
    "# Scatter plot:\n",
    "\n",
    "plt.figure(0)\n",
    "plt.axis([-3, 3, -3, 3])\n",
    "\n",
    "plt.title('data')\n",
    "colors = ['b', 'r']\n",
    "labels = ['0', '1']\n",
    "\n",
    "for i, ech in enumerate(echs2):\n",
    "    x, y = ech.numpy()[:, 0], ech.numpy()[:, 1]\n",
    "    plt.scatter(x, y, color=colors[i])\n",
    "\n",
    "plt.legend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d2f4c8",
   "metadata": {
    "id": "97d2f4c8"
   },
   "source": [
    "**Exercise 4:** \n",
    "\n",
    "What is the approximate best accuracy achievable with a model of class *P1*? \n",
    "\n",
    "Complete the *P3* class and train a model to achieve an accuracy of at least 90% on the training set. \n",
    "\n",
    "Why does the score after 50 epochs vary so much with each new training? \n",
    "\n",
    "Is it possible, with another class of model, to achieve 100% accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42117e",
   "metadata": {
    "deletable": false,
    "id": "7d42117e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bb7bc5459ad55cefff593dc6268083b",
     "grade": false,
     "grade_id": "exercise-4-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class P3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(P3, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # first layer: 2 neurons\n",
    "        self.fc2 = nn.Linear(2, 1)  # second layer: 1 neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        x = torch.cat((x, 1 - x), dim=1)  # output of sum 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e382771b",
   "metadata": {
    "deletable": false,
    "id": "e382771b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c55c5e939e35858bb4bcf76ce92104a",
     "grade": false,
     "grade_id": "exercise-4-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "loader2 = DataLoader(ds2, batch_size=10, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a8232",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "470a8232",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22064ec3571500c3c1960e0a6877849e",
     "grade": false,
     "grade_id": "exercise-4-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "838915d7-c7ea-4a7d-aeb8-8076a94cbad5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm # Progess bar\n",
    "lr = 0.1\n",
    "model = P3()\n",
    "# Two commonly used descent methods:\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    for x, labels in loader2:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "print(\"\\nTraining end\")\n",
    "# accuracy\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cdbad4",
   "metadata": {
    "deletable": false,
    "id": "c5cdbad4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "825a438ac87230317226ee974f5052a9",
     "grade": false,
     "grade_id": "exercise-4-4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class P9(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(P9, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,4) # a first layer with 4 neurons\n",
    "        self.fc2 = nn.Linear(4,4) # a second layer with 4 neurons\n",
    "        self.fc3 = nn.Linear(4,1) # a third layer with 1 neuron\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06efc4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "e06efc4f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3b4448662a871a03ff6346959cb7ac1",
     "grade": false,
     "grade_id": "exercise-4-5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "988fc1d3-a868-47a3-a492-0ed0ac9b5661",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = P9()\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f\"Accuracy:{acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c04e0",
   "metadata": {
    "id": "0a7c04e0"
   },
   "source": [
    "As the complexity of the model increases, the boundaries can better adapt to the **training set**.\n",
    "\n",
    "**C.** Now let's see how to generalize the approach to multiple classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ff916",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "187ff916",
    "outputId": "0b4d912d-658c-4e55-eb2d-3fb470116543",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "std = 0.5\n",
    "# Sample 0:\n",
    "mean0 = torch.tensor((-1., -1.))\n",
    "ech0 = mean0 + std * torch.randn(n, 2)\n",
    "\n",
    "# Sample 1:\n",
    "mean1 = torch.tensor((1., -1.))\n",
    "ech1 = mean1 + std * torch.randn(n, 2)\n",
    "\n",
    "# Sample 2:\n",
    "mean2 = torch.tensor((0., 1.))\n",
    "ech2 = mean2 + std * torch.randn(n, 2)\n",
    "\n",
    "echs3 = [ech0, ech1, ech2]\n",
    "\n",
    "# Scatter plot:\n",
    "plt.figure(0)\n",
    "plt.axis([-3, 3, -3, 3])\n",
    "\n",
    "plt.title('data')\n",
    "colors = ['b', 'r', 'g']\n",
    "labels = ['0', '1', '2']\n",
    "\n",
    "for i, ech in enumerate(echs):\n",
    "    x, y = ech.numpy()[:, 0], ech.numpy()[:, 1]\n",
    "    plt.scatter(x, y, color=colors[i])\n",
    "\n",
    "plt.legend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32cbc0",
   "metadata": {
    "id": "0f32cbc0"
   },
   "source": [
    "To separate these points, a two-layer perceptron should be sufficient. The problem is defining the cost function. To continue using the log-likelihood, the output layer will have as many neurons as classes. To define a probability distribution, normalized exponentials are used (softmax function):\n",
    "\n",
    "$p_i = \\dfrac{e^{y_i}}{\\sum{e^{y_j}}}$\n",
    "  \n",
    "Where the $y_i$ are the outputs of the neurons in the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320c067",
   "metadata": {
    "id": "d320c067"
   },
   "outputs": [],
   "source": [
    "class P6(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(P6, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # First layer: 2 neurons\n",
    "        self.fc2 = nn.Linear(2, 3)  # Second layer: 3 neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = self.fc1(x)\n",
    "        x = x.relu()\n",
    "        x = self.fc2(x)\n",
    "        x = x.softmax(dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ccf0e",
   "metadata": {
    "id": "832ccf0e"
   },
   "source": [
    "**Exercise 5:** Create datasets and dataloaders objects and check if it is possible to separate the points (accuracy > 95%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f403e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "93f403e4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2098c6d09146eb711fd7f230f1fb2f40",
     "grade": false,
     "grade_id": "exercise-5-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "12ab6c37-9de6-4825-e897-8fa354aaec0a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(inputs3.shape)\n",
    "print(targets3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44077195",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "44077195",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e533efa116d03805c84c1743fe3f45ba",
     "grade": false,
     "grade_id": "exercise-5-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "899f1725-3908-40db-a046-5f31659ed22b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = P6()\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f\"Accuracy:{acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
